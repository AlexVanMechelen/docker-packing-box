#!/usr/bin/python3
# -*- coding: UTF-8 -*-
from pbox import *
from pbox.__info__ import *
from tinyscript import *


__version__     = "2.1.2"
__doc__         = """
This tool aims to manipulate a dataset in multiple ways by:
- adding new executables from the packing-box Docker image or from a user-defined source, packed or not with the
   selected packers installed in the image.
- updating it with already-packed or not-packed executables given their labels or not (detection can be applied).
- fixing its content.
"""
__examples__    = [
    "convert my-dataset --new-name my-fileless-dataset",
    "export my-dataset --destination my-dataset-files -n 100",
    "features my-dataset",
    "list --show-all",
    "make my-dataset -f ELF -n 500 --source-dir /path/to/dotnet/exe",
    "make my-dataset -f PE,ELF -n 2000",
    "merge my-dataset dataset-to-be-merged",
    "plot my-dataset byte_0_after_ep byte_1_after_ep",
    "plot my-dataset number_resources --output-format png",
    "select my-dataset my-new-dataset --query \"label == 'upx'\"",
    "show my-dataset --per-format --limit 20",
    "update my-dataset --detect --refresh -s /path1/to/exe -s /path2/to/exe",
    "update my-dataset --source-dir /path/to/exe --labels /path/to/exe/labels.json",
    "view my-dataset --query \"ctime > 2020\"",
    "view my-dataset --query \"'PE32' in format\"",
]
__description__ = "Make datasets of packed and not packed executables for use with Machine Learning"


def __add_name(parser, force=False, name="name", help="name of the dataset"):
    def dataset_exists(string):
        if string == "all":
            return string
        p = config['datasets'].joinpath(string)
        (ts.folder_exists_or_create if force else ts.folder_exists)(str(p))
        if not force and not Dataset.check(string) and not FilelessDataset.check(string):
            raise ValueError("Bad dataset")
        return p
    parser.add_argument(name, type=dataset_exists, help=help)
    return parser


def _percentage(p):
    try:
        if 0. <= float(p) <= 1.:
            return p
    except ValueError:
        pass
    return ValueError("Not a percentage")


if __name__ == '__main__':
    sparsers = parser.add_subparsers(dest="command", help="command to be executed")
    alter = __add_name(sparsers.add_parser("alter", help="alter the target dataset with a set of transformations"))
    alter.add_argument("-a", "--modifiers-set", type=ts.file_exists, default=str(config['modifiers']),
                       help="modifiers set's YAML definition")
    alter.add_argument("-p", "--percentage", type=_percentage, default=.1, help="percentage of samples to be altered")
    convert = __add_name(sparsers.add_parser("convert", help="convert the target dataset to a fileless one"))
    convert.add_argument("-f", "--features-set", type=ts.file_exists, default=str(config['features']),
                         help="features set's YAML definition")
    convert.add_argument("-n", "--new-name", help="name for the new converted dataset",
                         note="if None, the original dataset is overwritten")
    __add_name(sparsers.add_parser("edit", help="edit the data file"))
    export = __add_name(sparsers.add_parser("export", help="export the packed executable from a dataset"))
    export.add_argument("-d", "--destination", default="export", type=ts.folder_does_not_exist,
                        help="destination folder for the exported executables")
    export.add_argument("-n", "--number-executables", dest="n", type=ts.pos_int, default=0,
                        help="number of packed executables to be exported")
    feat = __add_name(sparsers.add_parser("features", help="compute and view features"))
    feat.add_argument("-f", "--features-set", type=ts.file_exists, default=str(config['features']),
                      help="features set's YAML definition")
    fix = __add_name(sparsers.add_parser("fix", help="fix a corrupted dataset"))
    fgroup = fix.add_mutually_exclusive_group()
    fgroup.add_argument("-d", "--detect", action="store_true", help="detect used packer with installed detectors")
    fgroup.add_argument("-l", "--labels", type=ts.json_config, help="set labels from a JSON file")
    listds = sparsers.add_parser("list", help="list all the datasets from the workspace")
    listds.add_argument("-a", "--show-all", action="store_true", help="show all datasets even those that are corrupted")
    listds.add_argument("-f", "--hide-files", action="store_true", help="hide the 'files' column")
    listds.add_argument("-r", "--raw", action="store_true", help="display unformatted text", note="useful with grep")
    make = __add_name(sparsers.add_parser("make", help="add n randomly chosen executables from input sources to the "
                                                       "dataset"), True)
    make_ = make.add_mutually_exclusive_group()
    make_.add_argument("-a", "--pack-all", action="store_true", help="pack all executables")
    make_.add_argument("-b", "--balance", action="store_true", help="balance the dataset relatively to the number of "
                                                                    "packers used, not between packed and not packed")
    make.add_argument("-f", "--formats", type=ts.values_list, default="All", help="list of formats to be considered")
    make.add_argument("-n", "--number", dest="n", type=ts.pos_int, default=100,
                      help="number of executables for the output dataset")
    make.add_argument("-p", "--packer", action="extend", nargs="*", type=lambda p: Packer.get(p),
                      help="packer to be used")
    make.add_argument("-s", "--source-dir", action="extend", nargs="*", type=lambda p: ts.Path(p, expand=True),
                      help="executables source directory to be included")
    merge = __add_name(sparsers.add_parser("merge", help="merge two datasets"))
    __add_name(merge, name="name2", help="name of the dataset to merge")
    merge.add_argument("-n", "--new-name", help="name for the new merged dataset",
                       note="if None, the original dataset is overwritten")
    plot = __add_name(sparsers.add_parser("plot", help="plot the distribution of a given feature or multiple features "
                                                       "combined"))
    plot.add_argument("feature", action="extend", nargs="*", help="feature identifiers")
    plot.add_argument("-m", "--multiclass", action="store_true", help="process features using multiple label classes")
    plot.add_argument("-o", "--output-format", choices=["jpg", "png", "tif", "svg"],
                      help="image file format for plotting", note="no format means plotting in the terminal")
    purge = __add_name(sparsers.add_parser("purge", help="purge a dataset"), True)
    purge.add_argument("-b", "--backup", action="store_true", help="only purge backups")
    remove = __add_name(sparsers.add_parser("remove", help="remove executables from a dataset"))
    remove.add_argument("-q", "--query", help="query for selecting records to be removed",
                        note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    rename = __add_name(sparsers.add_parser("rename", help="rename a dataset"))
    rename.add_argument("name2", type=ts.folder_does_not_exist, help="new name of the dataset")
    __add_name(sparsers.add_parser("revert", help="revert a dataset to its previous state"))
    select = __add_name(sparsers.add_parser("select", help="select a subset of the dataset"))
    select.add_argument("name2", type=ts.folder_does_not_exist, help="name of the new dataset")
    select.add_argument("-n", "--number", dest="limit", type=ts.pos_int, default=0,
                        help="limit number of executables for the output dataset", note="0 means all")
    select.add_argument("-q", "--query", help="query for selecting records to be selected",
                        note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    show = __add_name(sparsers.add_parser("show", help="get an overview of the dataset"))
    show.add_argument("-l", "--limit", default=10, type=int, help="number of executables to be displayed per format")
    show.add_argument("--per-format", action="store_true", help="display statistics per format")
    update = __add_name(sparsers.add_parser("update", help="update a dataset with new executables"), True)
    ugroup = update.add_mutually_exclusive_group()
    ugroup.add_argument("-d", "--detect", action="store_true", help="detect used packer with installed detectors")
    ugroup.add_argument("-l", "--labels", type=ts.json_config, help="set labels from a JSON file")
    update.add_argument("-n", "--number", dest="n", type=ts.pos_int, default=0,
                        help="number of executables for the output dataset", note="0 means all")
    update.add_argument("-r", "--refresh", action="store_true", help="refresh labels of already existing executables")
    update.add_argument("-s", "--source-dir", action="extend", nargs="*", type=lambda p: ts.Path(p, expand=True),
                        help="executables source directory to be included")
    view = __add_name(sparsers.add_parser("view", help="view executables filtered from a dataset"))
    view.add_argument("-q", "--query", help="query for selecting records to be viewed",
                      note="see <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html>")
    initialize(noargs_action="help")
    logger.name = "dataset"
    logging.configLogger(logger, ["INFO", "DEBUG"][args.verbose], fmt=LOG_FORMATS[args.verbose], relative=True)
    name, args.load = getattr(args, "name", None), args.command not in ["list", "purge"]
    if args.command == "purge" and name == "all":
        purged = False
        for ds in Dataset.iteritems():
            ds.purge()
            purged = True
        if not purged:
            logger.warning("No dataset to purge in workspace (%s)" % str(config['datasets']))
    else:
        if hasattr(args, "modifiers_set"):
            Modifiers.source = args.modifiers_set  # configure the modifiers set's YAML definition source
        if hasattr(args, "features_set"):
            Features.source = args.features_set  # configure the features set's YAML definition source
        ds = (FilelessDataset if getattr(args, "fileless", False) or \
              name and FilelessDataset.check(name) else Dataset)(**vars(args))
        getattr(ds, args.command)(**vars(args))
        # it may occur that packing fails with a silenced error and that the related executable file remains in the
        #  files subfolder of the dataset while not handled in data.csv, hence creating an inconsistency ;
        #  fixing the dataset right after the make command allows to avoid this inconsistency
        if args.command == "make":
            ds.fix()

